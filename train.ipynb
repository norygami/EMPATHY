{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnorygano\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nory/projects/discollama/wandb/run-20240429_001450-h3dlhcy7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/norygano/TRACHI_Llama/runs/h3dlhcy7' target=\"_blank\">snowy-waterfall-45</a></strong> to <a href='https://wandb.ai/norygano/TRACHI_Llama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/norygano/TRACHI_Llama' target=\"_blank\">https://wandb.ai/norygano/TRACHI_Llama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/norygano/TRACHI_Llama/runs/h3dlhcy7' target=\"_blank\">https://wandb.ai/norygano/TRACHI_Llama/runs/h3dlhcy7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 493 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 62\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/62 00:25 < 25:19, 0.04 it/s, Epoch 1.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.291800</td>\n",
       "      <td>3.483510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.306700</td>\n",
       "      <td>3.153913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Setup Wandb\u001b[39;00m\n\u001b[1;32m    118\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRACHI_Llama\u001b[39m\u001b[38;5;124m'\u001b[39m, entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorygano\u001b[39m\u001b[38;5;124m'\u001b[39m, config\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mto_dict())\n\u001b[0;32m--> 120\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Finish Wandb session\u001b[39;00m\n\u001b[1;32m    123\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:361\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 361\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:518\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer_callback.py:374\u001b[0m, in \u001b[0;36mCallbackHandler.on_train_end\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mon_train_end\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/trainer_callback.py:415\u001b[0m, in \u001b[0;36mCallbackHandler.call_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 415\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/notebook.py:373\u001b[0m, in \u001b[0;36mNotebookProgressCallback.on_train_end\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, state, control, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_tracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_tracker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/notebook.py:161\u001b[0m, in \u001b[0;36mNotebookProgressBar.update\u001b[0;34m(self, value, force_update, comment)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_time_per_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_time_per_item \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m-\u001b[39m value)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_bar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_time \u001b[38;5;241m=\u001b[39m current_time\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/transformers/utils/notebook.py:183\u001b[0m, in \u001b[0;36mNotebookProgressBar.update_bar\u001b[0;34m(self, value, comment)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, +inf it/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maverage_time_per_item\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m it/s\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomment\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Fine-Tune (Unsloth)\n",
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "\n",
    "# Unload Model if it's active\n",
    "ollama.generate(model='TRACHI', keep_alive=0)\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"norygano/TRACHI\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"Llama-3-TRACHI-8B-Instruct\"\n",
    "\n",
    "# Constants\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "max_seq_length = 4096\n",
    "\n",
    "# Load your dataset\n",
    "raw_dataset = load_dataset(\"norygano/TRACHI\")\n",
    "\n",
    "# Load model + tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_id,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Function to apply chat template to each entry in the dataset\n",
    "def apply_chat_template(batch):\n",
    "    formatted_chats = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in batch['chat']]\n",
    "    return {'formatted_chat': formatted_chats}\n",
    "\n",
    "# Applying chat template to the dataset\n",
    "formatted_dataset = raw_dataset.map(apply_chat_template, batched=True)\n",
    "\n",
    "# Creating ConstantLengthDataset\n",
    "train_dataset = ConstantLengthDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=formatted_dataset['train'],\n",
    "    dataset_text_field='formatted_chat',\n",
    "    seq_length=max_seq_length,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "eval_dataset = ConstantLengthDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=formatted_dataset['test'],\n",
    "    dataset_text_field='formatted_chat',\n",
    "    seq_length=max_seq_length,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    "    max_seq_length = max_seq_length,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    gradient_accumulation_steps = 4,\n",
    "    warmup_steps = 5,\n",
    "    max_steps = -1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=4e-4,\n",
    "    num_train_epochs=2,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_steps = 1,\n",
    "    output_dir = \"outputs\",\n",
    "    evaluation_strategy='steps',\n",
    "    optim = \"adamw_8bit\",\n",
    "    tf32=True,\n",
    "    neftune_noise_alpha=5,\n",
    "    seed = 3407,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field = \"formatted_chat\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = args\n",
    "    )\n",
    "\n",
    "# Setup Wandb\n",
    "wandb.init(project='TRACHI_Llama', entity='norygano', config=args.to_dict())\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Finish Wandb session\n",
    "wandb.finish()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You're not saving a tokenizer as well?\n",
      "You can do it separately via `tokenizer.save_pretrained(...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 35.13 out of 54.92 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 27.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"Llama-3-TRACHI-8B-Instruct\", save_method = \"merged_16bit\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.fp16.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model file Llama-3-TRACHI-8B-Instruct/model-00001-of-00004.safetensors\n",
      "Loading model file Llama-3-TRACHI-8B-Instruct/model-00001-of-00004.safetensors\n",
      "Loading model file Llama-3-TRACHI-8B-Instruct/model-00002-of-00004.safetensors\n",
      "Loading model file Llama-3-TRACHI-8B-Instruct/model-00003-of-00004.safetensors\n",
      "Loading model file Llama-3-TRACHI-8B-Instruct/model-00004-of-00004.safetensors\n",
      "params = Params(n_vocab=128256, n_embd=4096, n_layer=32, n_ctx=8192, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('Llama-3-TRACHI-8B-Instruct'))\n",
      "Loaded vocab file PosixPath('Llama-3-TRACHI-8B-Instruct/tokenizer.json'), type 'bpe'\n",
      "Vocab info: <BpeVocab with 128000 base tokens and 256 added tokens>\n",
      "Special vocab info: <SpecialVocab with 280147 merges, special tokens {'bos': 128000, 'eos': 128009, 'pad': 128001}, add special tokens unset>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [128256, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [128256, 4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "Writing Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.fp16.bin, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 280147 merge(s).\n",
      "gguf: Setting special token type bos to 128000\n",
      "gguf: Setting special token type eos to 128009\n",
      "gguf: Setting special token type pad to 128001\n",
      "gguf: Setting chat_template to {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}{% endif %}\n",
      "[  1/291] Writing tensor token_embd.weight                      | size 128256 x   4096  | type F16  | T+   2\n",
      "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
      "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
      "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 20/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 21/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 22/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 23/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 24/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 25/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 26/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 27/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 28/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 29/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
      "[ 30/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   3\n",
      "[ 31/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 32/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   3\n",
      "[ 33/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
      "[ 34/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
      "[ 35/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 36/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
      "[ 37/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 38/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 39/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
      "[ 40/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 41/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 42/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[ 43/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 44/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 45/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 46/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 47/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 48/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
      "[ 49/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 50/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 51/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
      "[ 52/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 53/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 54/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
      "[ 55/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
      "[ 56/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
      "[ 57/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
      "[ 58/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   4\n",
      "[ 59/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 60/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
      "[ 61/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 62/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 63/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 64/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 65/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[ 66/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 67/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 68/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 69/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
      "[ 70/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 71/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 72/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 73/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 74/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+   5\n",
      "[ 75/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 76/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 77/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 78/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
      "[ 79/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 80/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 81/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 82/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 83/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+   5\n",
      "[ 84/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[ 85/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[ 86/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[ 87/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[ 88/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 89/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 90/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 91/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 92/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[ 93/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[ 94/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[ 95/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[ 96/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[ 97/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[ 98/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[ 99/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   6\n",
      "[100/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[101/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+   6\n",
      "[102/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   6\n",
      "[103/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   6\n",
      "[104/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   6\n",
      "[105/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+   6\n",
      "[106/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   6\n",
      "[107/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+   6\n",
      "[108/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[109/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[110/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[111/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[112/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[113/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[114/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[115/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[116/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[117/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[118/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[119/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[120/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[121/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[122/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   7\n",
      "[123/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+   7\n",
      "[124/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[125/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+   7\n",
      "[126/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   7\n",
      "[127/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   7\n",
      "[128/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+   7\n",
      "[129/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   7\n",
      "[130/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   7\n",
      "[131/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[132/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[133/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[134/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[135/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[136/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[137/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[138/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[139/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[140/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[141/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+   8\n",
      "[142/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[143/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+   8\n",
      "[144/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   8\n",
      "[145/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   8\n",
      "[146/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+   8\n",
      "[147/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   8\n",
      "[148/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   8\n",
      "[149/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   8\n",
      "[150/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[151/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[152/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[153/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[154/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[155/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[156/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[157/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   9\n",
      "[158/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   9\n",
      "[159/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+   9\n",
      "[160/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[161/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+   9\n",
      "[162/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   9\n",
      "[163/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   9\n",
      "[164/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+   9\n",
      "[165/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+   9\n",
      "[166/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10\n",
      "[167/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  10\n",
      "[168/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  10\n",
      "[169/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[170/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  10\n",
      "[171/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  10\n",
      "[172/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  10\n",
      "[173/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  10\n",
      "[174/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[175/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  11\n",
      "[176/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  11\n",
      "[177/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  11\n",
      "[178/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
      "[179/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  11\n",
      "[180/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  11\n",
      "[181/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  12\n",
      "[182/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
      "[183/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  12\n",
      "[184/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12\n",
      "[185/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
      "[186/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  12\n",
      "[187/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  12\n",
      "[188/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  12\n",
      "[189/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  12\n",
      "[190/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  12\n",
      "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  12\n",
      "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  12\n",
      "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  12\n",
      "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  12\n",
      "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  13\n",
      "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  13\n",
      "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  13\n",
      "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  13\n",
      "[200/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  13\n",
      "[201/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  13\n",
      "[202/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  13\n",
      "[203/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  13\n",
      "[204/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  14\n",
      "[205/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  14\n",
      "[206/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  14\n",
      "[207/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  14\n",
      "[208/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  14\n",
      "[209/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  14\n",
      "[210/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  14\n",
      "[211/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  14\n",
      "[212/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  14\n",
      "[213/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[214/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
      "[215/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[216/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[217/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
      "[218/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  15\n",
      "[219/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  15\n",
      "[220/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  15\n",
      "[221/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  15\n",
      "[222/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  15\n",
      "[223/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
      "[224/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[225/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
      "[226/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  16\n",
      "[227/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  16\n",
      "[228/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  16\n",
      "[229/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  16\n",
      "[230/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  16\n",
      "[231/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  16\n",
      "[232/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  16\n",
      "[233/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  16\n",
      "[234/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
      "[235/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  17\n",
      "[236/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  17\n",
      "[237/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  17\n",
      "[238/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  17\n",
      "[239/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  17\n",
      "[240/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  17\n",
      "[241/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  17\n",
      "[242/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  17\n",
      "[243/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  17\n",
      "[244/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  17\n",
      "[245/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  18\n",
      "[246/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  18\n",
      "[247/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  18\n",
      "[248/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  18\n",
      "[249/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  18\n",
      "[250/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  18\n",
      "[251/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  18\n",
      "[252/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  18\n",
      "[253/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  18\n",
      "[254/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  18\n",
      "[255/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  18\n",
      "[256/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  19\n",
      "[257/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  19\n",
      "[258/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  19\n",
      "[259/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  19\n",
      "[260/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  19\n",
      "[261/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  19\n",
      "[262/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  19\n",
      "[263/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  19\n",
      "[264/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  19\n",
      "[265/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  20\n",
      "[266/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  21\n",
      "[267/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  21\n",
      "[268/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  21\n",
      "[269/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  21\n",
      "[270/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  21\n",
      "[271/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  22\n",
      "[272/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  22\n",
      "[273/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  22\n",
      "[274/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  22\n",
      "[275/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  22\n",
      "[276/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n",
      "[277/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  22\n",
      "[278/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  22\n",
      "[279/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  22\n",
      "[280/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  22\n",
      "[281/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  22\n",
      "[282/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  23\n",
      "[283/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  26\n",
      "[284/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  26\n",
      "[285/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  26\n",
      "[286/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  26\n",
      "[287/291] Writing tensor output.weight                          | size 128256 x   4096  | type F16  | T+  26\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
      "[289/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  27\n",
      "[290/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  28\n",
      "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  28\n",
      "Wrote Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.fp16.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main: build = 2736 (dba497e0)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.fp16.bin' to 'Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.Q5_K_M.gguf' as Q5_K_M\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from Llama-3-TRACHI-8B-Instruct/Llama-3-TRACHI-8B-Instruct.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "[   1/ 291]                    token_embd.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q5_K .. size =  1002.00 MiB ->   344.44 MiB\n",
      "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  20/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  21/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  22/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  23/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  24/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  25/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  26/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  27/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  28/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  29/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  30/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  31/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  32/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  33/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  34/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  35/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  36/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  37/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  38/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  40/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  41/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  42/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  44/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  45/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  46/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  47/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  48/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  49/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  50/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  51/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  56/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  57/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  58/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  59/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  60/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  61/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  62/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  63/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  64/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  65/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  66/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  67/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  68/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  69/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  70/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  71/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  72/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  73/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  74/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  75/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  76/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  77/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  78/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  79/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  80/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  81/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  82/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  83/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  84/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[  85/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  86/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  87/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  89/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  90/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  91/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[  92/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  94/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  95/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[  96/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  97/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[  98/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[  99/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 100/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 101/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 102/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 103/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 104/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 105/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 106/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 107/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 108/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 109/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 110/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 111/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 112/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 113/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 114/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 115/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 116/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 117/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 118/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 119/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 123/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 124/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 125/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 126/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 127/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 128/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 129/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 130/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 131/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 132/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 133/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 134/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 135/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 136/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 137/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 139/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 140/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 141/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 143/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 144/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 145/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 146/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 147/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 148/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 149/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 150/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 151/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 152/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 153/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 154/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 155/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 156/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 157/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 158/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 159/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 160/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 161/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 162/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 163/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 164/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 168/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 169/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 170/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 171/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 172/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 174/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 175/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 176/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 177/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 178/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 179/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 180/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 181/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 182/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 183/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 184/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 185/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 186/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 187/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 188/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 189/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 190/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 200/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 201/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 202/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 203/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 204/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 205/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 206/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 207/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 208/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 209/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 210/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 211/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 212/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 213/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 214/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 215/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 216/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 217/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 218/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 219/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 220/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 221/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 222/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 223/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 224/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 225/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 226/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 227/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 228/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 229/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 230/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 231/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 232/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 233/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 234/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 235/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 236/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 237/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 238/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 239/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 240/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 242/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 243/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 244/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 245/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 247/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 248/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 249/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 250/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 251/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 252/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 253/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 254/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 255/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 256/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 257/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 258/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 259/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 260/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 261/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 262/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 263/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 264/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 265/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 266/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 267/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 268/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 269/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 270/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 271/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 272/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 273/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 274/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 275/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 276/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 277/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 278/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 279/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 280/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 281/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 282/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q5_K .. size =   112.00 MiB ->    38.50 MiB\n",
      "[ 283/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q5_K .. size =     8.00 MiB ->     2.75 MiB\n",
      "[ 284/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 285/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q5_K .. size =    32.00 MiB ->    11.00 MiB\n",
      "[ 286/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q6_K .. size =     8.00 MiB ->     3.28 MiB\n",
      "[ 287/ 291]                        output.weight - [ 4096, 128256,     1,     1], type =    f16, converting to q6_K .. size =  1002.00 MiB ->   410.98 MiB\n",
      "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q6_K .. size =   112.00 MiB ->    45.94 MiB\n",
      "[ 290/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "llama_model_quantize_internal: model size  = 15317.02 MB\n",
      "llama_model_quantize_internal: quant size  =  5459.93 MB\n",
      "\n",
      "main: quantize time = 54317.99 ms\n",
      "main:    total time = 54317.99 ms\n"
     ]
    }
   ],
   "source": [
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "# Quantize\n",
    "import os\n",
    "QUANTIZATION_METHODS = [\"q5_k_m\"]\n",
    "new_model = \"Llama-3-TRACHI-8B-Instruct\"\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{new_model}.fp16.bin\"\n",
    "model_path = os.path.join(new_model, fp16)\n",
    "print(model_path)\n",
    "!python llama.cpp/convert.py {new_model} --outtype f16 --outfile {model_path} --pad-vocab --vocab-type bpe\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{new_model}/{new_model}.{method.upper()}.gguf\"\n",
    "    !llama.cpp/quantize {model_path} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nory/projects/discollama/modelfiles/Modelfile_TRACHI_L\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update Modelfile\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'modelfiles', 'Modelfile_TRACHI_L')\n",
    "print(path)\n",
    "ollama.create(model='TRACHI', path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push -> HF\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "api = HfApi()\n",
    "\n",
    "new_model = \"Llama-3-TRACHI-8B-Instruct\"\n",
    "#api.create_repo(f'{new_model}-GGUF')\n",
    "\n",
    "# Upload gguf files\n",
    "api.upload_folder(\n",
    "    folder_path=new_model,\n",
    "    repo_id=f\"norygano/{new_model}-GGUF\",\n",
    "    allow_patterns=f\"*.gguf\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload (FP16) -> merge w/ LoRA weights\n",
    "from datetime import datetime\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    #MistralForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "if 'pipe' in locals():\n",
    "  del pipe\n",
    "if 'trainer' in locals():\n",
    "  del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Reload the base model in bf16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Assuming PeftModel is a custom or previously defined model class for handling post-training operations\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer consistent with the first step and apply the same configurations\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The following lines were commented out in the first step but included here for consistency\n",
    "# Uncomment and adjust if necessary based on your specific requirements\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# This setting was not changed in the first step, but keep it if needed for your use case\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "formatted_now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the formatted date and time\n",
    "print(\"Current Date and Time:\", formatted_now)\n",
    "\n",
    "model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "# Quantize\n",
    "import os\n",
    "QUANTIZATION_METHODS = [\"q5_k_m\"]\n",
    "new_model = \"Llama-3-TRACHI-8B-Instruct\"\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{new_model}.fp16.bin\"\n",
    "model_path = os.path.join(new_model, fp16)\n",
    "print(model_path)\n",
    "!python llama.cpp/convert.py {new_model} --outtype f16 --outfile {model_path} --pad-vocab --vocab-type bpe\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{new_model}/{new_model}.{method.upper()}.gguf\"\n",
    "    !llama.cpp/quantize {model_path} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement batch training\n",
    "learning_rates = [4e-4, 4.5e-4, 5e-4]\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "for lr in learning_rates:\n",
    "\n",
    " # Load base model\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id,\n",
    "      attn_implementation=\"flash_attention_2\",\n",
    "      quantization_config=bnb_config,\n",
    "      device_map=device_map\n",
    "  )\n",
    "\n",
    "\n",
    "  # Initialize data collator\n",
    "  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, return_tensors=\"pt\", mlm=False)\n",
    "  \n",
    "  training_arguments = TrainingArguments(\n",
    "      output_dir=output_dir,\n",
    "      num_train_epochs=num_train_epochs,\n",
    "      per_device_train_batch_size=per_device_train_batch_size,\n",
    "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "      evaluation_strategy='steps',\n",
    "      optim=optim,\n",
    "      save_steps=save_steps,\n",
    "      logging_steps=logging_steps,\n",
    "      learning_rate=lr,\n",
    "      weight_decay=weight_decay,\n",
    "      fp16=fp16,\n",
    "      bf16=bf16,\n",
    "      max_grad_norm=max_grad_norm,\n",
    "      max_steps=max_steps,\n",
    "      warmup_ratio=warmup_ratio,\n",
    "      group_by_length=group_by_length,\n",
    "      lr_scheduler_type=lr_scheduler_type,\n",
    "      tf32=True,\n",
    "      neftune_noise_alpha=5,\n",
    "      report_to=\"wandb\"\n",
    "  )\n",
    "\n",
    "  # Set supervised fine-tuning parameters\n",
    "  trainer = SFTTrainer(\n",
    "      model=model,\n",
    "      train_dataset=dataset['train'],\n",
    "      eval_dataset=dataset['test'],\n",
    "      data_collator=data_collator,\n",
    "      peft_config=peft_config,\n",
    "      dataset_text_field=\"formatted_chat\",\n",
    "      max_seq_length=max_seq_length,\n",
    "      tokenizer=tokenizer,\n",
    "      args=training_arguments,\n",
    "      packing=packing,\n",
    "  )\n",
    "\n",
    "  # Setup Wandb\n",
    "  wandb.init(project='TRACHI_Llama', entity='norygano', config=training_arguments.to_dict())\n",
    "\n",
    "  # Start training and let SFTTrainer handle evaluation\n",
    "  trainer.train()\n",
    "\n",
    "  # Finish Wandb session\n",
    "  wandb.finish()\n",
    "\n",
    "  # Save trained model\n",
    "  trainer.model.save_pretrained(new_model)\n",
    "\n",
    "  # Cleanup\n",
    "  del model, trainer\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use unsloth Save when tokenizer is fixed\n",
    "model.save_pretrained_gguf(\"Llama-3-TRACHI-8B-Instruct\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
