{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'TRACHI',\n",
       " 'created_at': '2024-04-29T19:34:49.369966186Z',\n",
       " 'response': '',\n",
       " 'done': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "# Unload Model if it's active\n",
    "ollama.generate(model='TRACHI', keep_alive=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030ab0b7adec4fc9a4f52a0563ff47fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Mistral patching release 2024.4\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.2.2. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.25.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'quantization_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Load model + tokenizer\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFastLanguageModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mflash_attention_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39muse_flash_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     95\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/loader.py:138\u001b[0m, in \u001b[0;36mFastLanguageModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# In case the model supports tagging, add the unsloth tag.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_model_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/unsloth_env/lib/python3.10/site-packages/unsloth/models/mistral.py:361\u001b[0m, in \u001b[0;36mFastMistralModel.from_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m     bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m    354\u001b[0m         load_in_4bit              \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m         bnb_4bit_use_double_quant \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    356\u001b[0m         bnb_4bit_quant_type       \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    357\u001b[0m         bnb_4bit_compute_dtype    \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m max_position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(max_seq_length, model_max_seq_length)\n\u001b[0;32m--> 361\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    362\u001b[0m     model_name,\n\u001b[1;32m    363\u001b[0m     device_map          \u001b[38;5;241m=\u001b[39m device_map,\n\u001b[1;32m    364\u001b[0m     torch_dtype         \u001b[38;5;241m=\u001b[39m dtype,\n\u001b[1;32m    365\u001b[0m     quantization_config \u001b[38;5;241m=\u001b[39m bnb_config,\n\u001b[1;32m    366\u001b[0m     token               \u001b[38;5;241m=\u001b[39m token,\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# rope_scaling      = rope_scaling,\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     trust_remote_code   \u001b[38;5;241m=\u001b[39m trust_remote_code,\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Counteract saved tokenizers\u001b[39;00m\n\u001b[1;32m    373\u001b[0m tokenizer_name \u001b[38;5;241m=\u001b[39m model_name \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m tokenizer_name\n",
      "\u001b[0;31mTypeError\u001b[0m: transformers.models.auto.auto_factory._BaseAutoModelClass.from_pretrained() got multiple values for keyword argument 'quantization_config'"
     ]
    }
   ],
   "source": [
    "# Fine-Tune\n",
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    #MistralForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "\n",
    "# Unload Model if it's active\n",
    "ollama.generate(model='TRACHI', keep_alive=0)\n",
    "\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"cognitivecomputations/dolphin-2.8-mistral-7b-v02\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"norygano/TRACHI\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"dolphin-mistral-TRACHI-7b-v02\"\n",
    "\n",
    "# Constants\n",
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "# Sequence Length\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# bnb\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype = \"bfloat16\",\n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "  print('No Padding Token set by tokenizer. Setting padding token to eos_token.')\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# Function to apply chat template to each entry in the dataset\n",
    "def apply_chat_template(batch):\n",
    "    # Apply the chat template with `add_generation_prompt=False`\n",
    "    # Adjust the following line if your data structure is different\n",
    "    formatted_chats = [tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False) for chat in batch['chat']]\n",
    "    return {'formatted_chat': formatted_chats}\n",
    "\n",
    "# Applying chat template to the dataset\n",
    "dataset = dataset.map(apply_chat_template, batched=True)\n",
    "\n",
    "# Print some entries after applying chat template\n",
    "'''\n",
    "print(\"Sample entries after applying chat template:\")\n",
    "for i in range(3):  # Print the first 3 entries\n",
    "    print(dataset['train'][i]['formatted_chat'])\n",
    "'''\n",
    "\n",
    "# Tokenize the formatted chats\n",
    "def tokenize_function(batch):\n",
    "    # Ensure this line correctly handles your data's structure\n",
    "    return tokenizer(batch['formatted_chat'], padding=True, truncation=True, max_length=max_seq_length)\n",
    "\n",
    "# Applying tokenization\n",
    "dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load model + tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "model.use_flash_attention=True\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, return_tensors=\"pt\", mlm=False)\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "peft_config = LoraConfig(\n",
    "    r = 8,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    gradient_accumulation_steps = 8,\n",
    "    warmup_steps=10,\n",
    "    max_steps = -1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=2,\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),\n",
    "    bf16 = torch.cuda.is_bf16_supported(),\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps = 1,\n",
    "    output_dir = \"outputs\",\n",
    "    evaluation_strategy='epoch',\n",
    "    optim = \"adamw_8bit\",\n",
    "    tf32=True,\n",
    "    neftune_noise_alpha=5,\n",
    "    seed = 3407,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    dataset_text_field = \"formatted_chat\",\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = args\n",
    "    )\n",
    "\n",
    "# Setup Wandb\n",
    "wandb.init(project='TRACHI_Llama', entity='norygano', config=args.to_dict())\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Finish Wandb session\n",
    "wandb.finish()\n",
    "\n",
    "# Save trained model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Reload deps and vars\n",
    "from unsloth import FastLanguageModel\n",
    "import wandb\n",
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer import ConstantLengthDataset\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    #MistralForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from datasets import load_dataset\n",
    "import ollama\n",
    "\n",
    "# Unload Model if it's active\n",
    "ollama.generate(model='TRACHI', keep_alive=0)\n",
    "\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "model_id = \"cognitivecomputations/dolphin-2.9-llama3-8b\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "dataset_name = \"norygano/TRACHI\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "new_model = \"dolphin-llama-3-TRACHI-8b\"\n",
    "\n",
    "# Constants\n",
    "model_name = model_id.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62db570ebe604805bf019b7673633507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Date and Time: 2024-05-01 00:03:07\n"
     ]
    }
   ],
   "source": [
    "# Reload (FP16) -> merge w/ LoRA weights\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "if 'pipe' in locals():\n",
    "  del pipe\n",
    "if 'trainer' in locals():\n",
    "  del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# The model that you want to train from the Hugging Face hub\n",
    "#model_id = \"cognitivecomputations/dolphin-2.2.1-mistral-7b\"\n",
    "\n",
    "# The instruction dataset to use\n",
    "#dataset_name = \"norygano/TRACHI\"\n",
    "\n",
    "# Fine-tuned model name\n",
    "#new_model = \"dolphin-mistral-TRACHI-7b\"\n",
    "\n",
    "# Reload the base model in bf16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=False,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Assuming PeftModel is a custom or previously defined model class for handling post-training operations\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer consistent with the first step and apply the same configurations\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The following lines were commented out in the first step but included here for consistency\n",
    "# Uncomment and adjust if necessary based on your specific requirements\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# This setting was not changed in the first step, but keep it if needed for your use case\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a strin\n",
    "formatted_now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the formatted date and time\n",
    "print(\"Current Date and Time:\", formatted_now)\n",
    "\n",
    "model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.fp16.bin\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00001-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00001-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00002-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00003-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00004-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00005-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00006-of-00007.safetensors\n",
      "Loading model file dolphin-llama-3-TRACHI-8b/model-00007-of-00007.safetensors\n",
      "params = Params(n_vocab=128258, n_embd=4096, n_layer=32, n_ctx=8192, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=500000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('dolphin-llama-3-TRACHI-8b'))\n",
      "Loaded vocab file PosixPath('dolphin-llama-3-TRACHI-8b/tokenizer.json'), type 'bpe'\n",
      "Vocab info: <BpeVocab with 128000 base tokens and 258 added tokens>\n",
      "Special vocab info: <SpecialVocab with 280147 merges, special tokens {'bos': 128000, 'eos': 128256, 'pad': 128001}, add special tokens unset>\n",
      "Permuting layer 0\n",
      "Permuting layer 1\n",
      "Permuting layer 2\n",
      "Permuting layer 3\n",
      "Permuting layer 4\n",
      "Permuting layer 5\n",
      "Permuting layer 6\n",
      "Permuting layer 7\n",
      "Permuting layer 8\n",
      "Permuting layer 9\n",
      "Permuting layer 10\n",
      "Permuting layer 11\n",
      "Permuting layer 12\n",
      "Permuting layer 13\n",
      "Permuting layer 14\n",
      "Permuting layer 15\n",
      "Permuting layer 16\n",
      "Permuting layer 17\n",
      "Permuting layer 18\n",
      "Permuting layer 19\n",
      "Permuting layer 20\n",
      "Permuting layer 21\n",
      "Permuting layer 22\n",
      "Permuting layer 23\n",
      "Permuting layer 24\n",
      "Permuting layer 25\n",
      "Permuting layer 26\n",
      "Permuting layer 27\n",
      "Permuting layer 28\n",
      "Permuting layer 29\n",
      "Permuting layer 30\n",
      "Permuting layer 31\n",
      "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [128258, 4096]\n",
      "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | F32    | [4096, 14336]\n",
      "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | F32    | [14336, 4096]\n",
      "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | F32    | [14336, 4096]\n",
      "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | F32    | [1024, 4096]\n",
      "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | F32    | [4096, 4096]\n",
      "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | F32    | [4096, 4096]\n",
      "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | F32    | [1024, 4096]\n",
      "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
      "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | F32    | [4096, 14336]\n",
      "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | F32    | [14336, 4096]\n",
      "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | F32    | [14336, 4096]\n",
      "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | F32    | [1024, 4096]\n",
      "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | F32    | [4096, 4096]\n",
      "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | F32    | [4096, 4096]\n",
      "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | F32    | [1024, 4096]\n",
      "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
      "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
      "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
      "lm_head.weight                                   -> output.weight                            | BF16   | [128258, 4096]\n",
      "Writing dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.fp16.bin, format 1\n",
      "Ignoring added_tokens.json since model matches vocab size without it.\n",
      "gguf: This GGUF file is for Little Endian only\n",
      "gguf: Adding 280147 merge(s).\n",
      "gguf: Setting special token type bos to 128000\n",
      "gguf: Setting special token type eos to 128256\n",
      "gguf: Setting special token type pad to 128001\n",
      "gguf: Setting chat_template to {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "[  1/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
      "[  2/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
      "[  3/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
      "[  4/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[  5/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
      "[  6/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
      "[  7/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
      "[  8/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
      "[  9/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 10/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 11/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 12/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 13/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
      "[ 14/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
      "[ 15/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   5\n",
      "[ 16/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
      "[ 17/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   7\n",
      "[ 18/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
      "[ 19/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7\n",
      "[ 20/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7\n",
      "[ 21/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
      "[ 22/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   7\n",
      "[ 23/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   7\n",
      "[ 24/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   7\n",
      "[ 25/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
      "[ 26/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8\n",
      "[ 27/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
      "[ 28/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   8\n",
      "[ 29/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   9\n",
      "[ 30/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   9\n",
      "[ 31/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n",
      "[ 32/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
      "[ 33/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   9\n",
      "[ 34/291] Writing tensor token_embd.weight                      | size 128258 x   4096  | type F16  | T+  13\n",
      "[ 35/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[ 36/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
      "[ 37/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[ 38/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
      "[ 39/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[ 40/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
      "[ 41/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+  13\n",
      "[ 42/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
      "[ 43/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  14\n",
      "[ 44/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
      "[ 45/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
      "[ 46/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
      "[ 47/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  17\n",
      "[ 48/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  17\n",
      "[ 49/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  17\n",
      "[ 50/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  17\n",
      "[ 51/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  17\n",
      "[ 52/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  17\n",
      "[ 53/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+  17\n",
      "[ 54/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  17\n",
      "[ 55/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  17\n",
      "[ 56/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  24\n",
      "[ 57/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  24\n",
      "[ 58/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  24\n",
      "[ 59/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  24\n",
      "[ 60/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+  24\n",
      "[ 61/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  24\n",
      "[ 62/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  24\n",
      "[ 63/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  24\n",
      "[ 64/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  26\n",
      "[ 65/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  26\n",
      "[ 66/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  26\n",
      "[ 67/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  26\n",
      "[ 68/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  26\n",
      "[ 69/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  26\n",
      "[ 70/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  27\n",
      "[ 71/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  27\n",
      "[ 72/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  28\n",
      "[ 73/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
      "[ 74/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 75/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  28\n",
      "[ 76/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  28\n",
      "[ 77/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  28\n",
      "[ 78/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  28\n",
      "[ 79/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  28\n",
      "[ 80/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  29\n",
      "[ 81/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  29\n",
      "[ 82/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  29\n",
      "[ 83/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  29\n",
      "[ 84/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 85/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 86/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 87/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 88/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 89/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 90/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 91/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 92/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 93/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 94/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  29\n",
      "[ 95/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  29\n",
      "[ 96/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  31\n",
      "[ 97/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  31\n",
      "[ 98/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  31\n",
      "[ 99/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  31\n",
      "[100/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  32\n",
      "[101/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  32\n",
      "[102/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  32\n",
      "[103/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  32\n",
      "[104/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  32\n",
      "[105/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  32\n",
      "[106/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  34\n",
      "[107/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  34\n",
      "[108/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  35\n",
      "[109/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  35\n",
      "[110/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  35\n",
      "[111/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  35\n",
      "[112/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  35\n",
      "[113/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  38\n",
      "[114/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  41\n",
      "[115/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  42\n",
      "[116/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  42\n",
      "[117/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  42\n",
      "[118/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
      "[119/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  42\n",
      "[120/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  42\n",
      "[121/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  42\n",
      "[122/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  43\n",
      "[123/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  43\n",
      "[124/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  43\n",
      "[125/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  43\n",
      "[126/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  43\n",
      "[127/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  43\n",
      "[128/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  43\n",
      "[129/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  43\n",
      "[130/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  43\n",
      "[131/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  43\n",
      "[132/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  43\n",
      "[133/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[134/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[135/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[136/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[137/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[138/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[139/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[140/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[141/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  43\n",
      "[142/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  43\n",
      "[143/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  48\n",
      "[144/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  48\n",
      "[145/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  48\n",
      "[146/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  48\n",
      "[147/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  48\n",
      "[148/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  48\n",
      "[149/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  48\n",
      "[150/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  48\n",
      "[151/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  50\n",
      "[152/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  50\n",
      "[153/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  50\n",
      "[154/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  50\n",
      "[155/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  50\n",
      "[156/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  50\n",
      "[157/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  50\n",
      "[158/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  50\n",
      "[159/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  52\n",
      "[160/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  52\n",
      "[161/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  52\n",
      "[162/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  52\n",
      "[163/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  52\n",
      "[164/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  52\n",
      "[165/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  52\n",
      "[166/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  52\n",
      "[167/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  54\n",
      "[168/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  54\n",
      "[169/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  54\n",
      "[170/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  54\n",
      "[171/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  54\n",
      "[172/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  54\n",
      "[173/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  54\n",
      "[174/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  54\n",
      "[175/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  54\n",
      "[176/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  55\n",
      "[177/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
      "[178/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
      "[179/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  55\n",
      "[180/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
      "[181/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  55\n",
      "[182/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
      "[183/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
      "[184/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[185/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[186/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[187/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[188/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[189/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[190/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[191/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[192/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[193/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[194/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  55\n",
      "[195/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
      "[196/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  57\n",
      "[197/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  57\n",
      "[198/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  57\n",
      "[199/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  58\n",
      "[200/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  58\n",
      "[201/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  58\n",
      "[202/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  58\n",
      "[203/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  58\n",
      "[204/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  59\n",
      "[205/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  61\n",
      "[206/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  61\n",
      "[207/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  61\n",
      "[208/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  61\n",
      "[209/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  61\n",
      "[210/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  61\n",
      "[211/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  61\n",
      "[212/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  64\n",
      "[213/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  69\n",
      "[214/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  69\n",
      "[215/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  69\n",
      "[216/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  69\n",
      "[217/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  69\n",
      "[218/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  69\n",
      "[219/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  69\n",
      "[220/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  69\n",
      "[221/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  71\n",
      "[222/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  71\n",
      "[223/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  71\n",
      "[224/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  71\n",
      "[225/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  71\n",
      "[226/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  71\n",
      "[227/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  71\n",
      "[228/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  71\n",
      "[229/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  71\n",
      "[230/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  72\n",
      "[231/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
      "[232/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  72\n",
      "[233/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  72\n",
      "[234/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  72\n",
      "[235/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  72\n",
      "[236/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  72\n",
      "[237/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[238/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[239/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[240/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[241/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[242/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[243/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[244/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[245/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[246/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[247/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  72\n",
      "[248/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  72\n",
      "[249/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  74\n",
      "[250/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  74\n",
      "[251/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  74\n",
      "[252/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  74\n",
      "[253/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  74\n",
      "[254/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  75\n",
      "[255/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  75\n",
      "[256/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  75\n",
      "[257/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  75\n",
      "[258/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  75\n",
      "[259/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  76\n",
      "[260/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  76\n",
      "[261/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  76\n",
      "[262/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  77\n",
      "[263/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  77\n",
      "[264/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  77\n",
      "[265/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  77\n",
      "[266/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  77\n",
      "[267/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  78\n",
      "[268/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  79\n",
      "[269/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  79\n",
      "[270/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  79\n",
      "[271/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  79\n",
      "[272/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  79\n",
      "[273/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  79\n",
      "[274/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  79\n",
      "[275/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  80\n",
      "[276/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  80\n",
      "[277/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  80\n",
      "[278/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  80\n",
      "[279/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  80\n",
      "[280/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[281/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  80\n",
      "[282/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[283/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  80\n",
      "[284/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[285/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  80\n",
      "[286/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[287/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  80\n",
      "[288/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  80\n",
      "[289/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  80\n",
      "[290/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  80\n",
      "[291/291] Writing tensor output.weight                          | size 128258 x   4096  | type F16  | T+  86\n",
      "Wrote dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.fp16.bin\n",
      "main: build = 2762 (24affa7d)\n",
      "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
      "main: quantizing 'dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.fp16.bin' to 'dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.Q8_0.gguf' as Q8_0\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from dolphin-llama-3-TRACHI-8b/dolphin-llama-3-TRACHI-8b.fp16.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128258\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128258]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128258]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128258]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128256\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "[   1/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[   2/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[   3/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[   4/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[   5/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[   6/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[   7/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[   8/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[   9/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  10/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  11/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  12/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  13/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  14/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  15/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  16/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  17/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  18/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  19/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  20/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  21/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  22/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  23/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  24/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  25/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  26/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  27/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  28/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  29/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  30/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  31/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  32/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  33/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  34/ 291]                    token_embd.weight - [ 4096, 128258,     1,     1], type =    f16, converting to q8_0 .. size =  1002.02 MiB ->   532.32 MiB\n",
      "[  35/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  36/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  37/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  38/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  39/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  40/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  41/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  42/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  43/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  44/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  45/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  46/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  47/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  48/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  49/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  50/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  51/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  52/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  53/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  54/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  55/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  56/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  57/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  58/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  59/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  60/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  61/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  62/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  63/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  64/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  65/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  66/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  67/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  68/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  69/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  70/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  71/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  72/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  73/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  74/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  75/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  76/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  77/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  78/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  79/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  80/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  81/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  82/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[  83/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[  84/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  85/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  86/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  87/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  88/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  89/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  90/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  91/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  92/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  93/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  94/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  95/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[  96/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  97/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  98/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[  99/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 100/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 101/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 102/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 103/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 104/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 105/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 106/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 107/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 108/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 109/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 110/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 111/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 112/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 113/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 114/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 115/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 116/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 117/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 118/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 119/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 120/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 121/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 122/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 123/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 124/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 125/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 126/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 127/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 128/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 129/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 130/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 131/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 132/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 133/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 134/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 135/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 136/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 137/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 138/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 139/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 140/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 141/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 142/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 143/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 144/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 145/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 146/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 147/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 148/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 149/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 150/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 151/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 152/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 153/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 154/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 155/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 156/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 157/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 158/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 159/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 160/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 161/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 162/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 163/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 164/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 165/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 166/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 167/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 168/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 169/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 170/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 171/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 172/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 173/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 174/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 175/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 176/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 177/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 178/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 179/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 180/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 181/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 182/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 183/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 184/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 185/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 186/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 187/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 188/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 189/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 190/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 191/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 192/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 193/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 194/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 195/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 196/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 197/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 198/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 199/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 200/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 201/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 202/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 203/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 204/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 205/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 206/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 207/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 208/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 209/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 210/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 211/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 212/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 213/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 214/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 215/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 216/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 217/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 218/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 219/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 220/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 221/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 222/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 223/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 224/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 225/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 226/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 227/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 228/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 229/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 230/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 231/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 232/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 233/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 234/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 235/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 236/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 237/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 238/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 239/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 240/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 241/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 242/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 243/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 244/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 245/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 246/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 247/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 248/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 249/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 250/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 251/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 252/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 253/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 254/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 255/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 256/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 257/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 258/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 259/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 260/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 261/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 262/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 263/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 264/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 265/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 266/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 267/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 268/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 269/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 270/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 271/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 272/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 273/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 274/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 275/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, converting to q8_0 .. size =   112.00 MiB ->    59.50 MiB\n",
      "[ 276/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 277/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 278/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, converting to q8_0 .. size =    32.00 MiB ->    17.00 MiB\n",
      "[ 279/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, converting to q8_0 .. size =     8.00 MiB ->     4.25 MiB\n",
      "[ 280/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 281/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 282/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 283/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 284/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 285/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 286/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 287/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 288/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 289/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 290/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
      "[ 291/ 291]                        output.weight - [ 4096, 128258,     1,     1], type =    f16, converting to q8_0 .. size =  1002.02 MiB ->   532.32 MiB\n",
      "llama_model_quantize_internal: model size  = 15317.05 MB\n",
      "llama_model_quantize_internal: quant size  =  8137.66 MB\n",
      "\n",
      "main: quantize time = 49929.63 ms\n",
      "main:    total time = 49929.63 ms\n"
     ]
    }
   ],
   "source": [
    "# Quantize\n",
    "import os\n",
    "QUANTIZATION_METHODS = [\"q8_0\"]\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{new_model}.fp16.bin\"\n",
    "model_path = os.path.join(new_model, fp16)\n",
    "print(model_path)\n",
    "!python llama.cpp/convert.py {new_model} --outtype f16 --outfile {model_path} --vocab-type bpe\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{new_model}/{new_model}.{method.upper()}.gguf\"\n",
    "    !llama.cpp/quantize {model_path} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nory/projects/discollama/modelfiles/Modelfile_TRACHI_D2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update Modelfile\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'modelfiles', 'Modelfile_TRACHI_D2')\n",
    "print(path)\n",
    "ollama.create(model='TRACHI', path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d6cffd9b8544efa6f413f839be8a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Llama-3-TRACHI-8B-Instruct.Q5_K_M.gguf:   0%|          | 0.00/5.73G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/norygano/Llama-3-TRACHI-8B-Instruct-GGUF/commit/8275f7d8ea6316b2a61680716ce69133607bd5a9', commit_message='Upload folder using huggingface_hub', commit_description='', oid='8275f7d8ea6316b2a61680716ce69133607bd5a9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push -> HF\n",
    "from huggingface_hub import create_repo, HfApi\n",
    "api = HfApi()\n",
    "\n",
    "#api.create_repo(f'{new_model}-GGUF')\n",
    "\n",
    "# Upload gguf files\n",
    "api.upload_folder(\n",
    "    folder_path=new_model,\n",
    "    repo_id=f\"norygano/{new_model}-GGUF\",\n",
    "    allow_patterns=f\"*.gguf\",\n",
    "    repo_type=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload (FP16) -> merge w/ LoRA weights\n",
    "from datetime import datetime\n",
    "from peft import LoraConfig, PeftModel\n",
    "from transformers import (\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    #MistralForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "if 'pipe' in locals():\n",
    "  del pipe\n",
    "if 'trainer' in locals():\n",
    "  del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# Reload the base model in bf16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "print(len(tokenizer))\n",
    "\n",
    "# Assuming PeftModel is a custom or previously defined model class for handling post-training operations\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer consistent with the first step and apply the same configurations\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# The following lines were commented out in the first step but included here for consistency\n",
    "# Uncomment and adjust if necessary based on your specific requirements\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# This setting was not changed in the first step, but keep it if needed for your use case\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the date and time as a string\n",
    "formatted_now = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Print the formatted date and time\n",
    "print(\"Current Date and Time:\", formatted_now)\n",
    "\n",
    "model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup #CUDA-OOM\n",
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "# Quantize\n",
    "import os\n",
    "QUANTIZATION_METHODS = [\"q5_k_m\"]\n",
    "new_model = \"Llama-3-TRACHI-8B-Instruct\"\n",
    "\n",
    "# Convert to fp16\n",
    "fp16 = f\"{new_model}.fp16.bin\"\n",
    "model_path = os.path.join(new_model, fp16)\n",
    "print(model_path)\n",
    "!python llama.cpp/convert.py {new_model} --outtype f16 --outfile {model_path} --pad-vocab --vocab-type bpe\n",
    "\n",
    "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
    "for method in QUANTIZATION_METHODS:\n",
    "    qtype = f\"{new_model}/{new_model}.{method.upper()}.gguf\"\n",
    "    !llama.cpp/quantize {model_path} {qtype} {method}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement batch training\n",
    "learning_rates = [4e-4, 4.5e-4, 5e-4]\n",
    "\n",
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "for lr in learning_rates:\n",
    "\n",
    " # Load base model\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id,\n",
    "      attn_implementation=\"flash_attention_2\",\n",
    "      quantization_config=bnb_config,\n",
    "      device_map=device_map\n",
    "  )\n",
    "\n",
    "\n",
    "  # Initialize data collator\n",
    "  data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, return_tensors=\"pt\", mlm=False)\n",
    "  \n",
    "  training_arguments = TrainingArguments(\n",
    "      output_dir=output_dir,\n",
    "      num_train_epochs=num_train_epochs,\n",
    "      per_device_train_batch_size=per_device_train_batch_size,\n",
    "      gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "      evaluation_strategy='steps',\n",
    "      optim=optim,\n",
    "      save_steps=save_steps,\n",
    "      logging_steps=logging_steps,\n",
    "      learning_rate=lr,\n",
    "      weight_decay=weight_decay,\n",
    "      fp16=fp16,\n",
    "      bf16=bf16,\n",
    "      max_grad_norm=max_grad_norm,\n",
    "      max_steps=max_steps,\n",
    "      warmup_ratio=warmup_ratio,\n",
    "      group_by_length=group_by_length,\n",
    "      lr_scheduler_type=lr_scheduler_type,\n",
    "      tf32=True,\n",
    "      neftune_noise_alpha=5,\n",
    "      report_to=\"wandb\"\n",
    "  )\n",
    "\n",
    "  # Set supervised fine-tuning parameters\n",
    "  trainer = SFTTrainer(\n",
    "      model=model,\n",
    "      train_dataset=dataset['train'],\n",
    "      eval_dataset=dataset['test'],\n",
    "      data_collator=data_collator,\n",
    "      peft_config=peft_config,\n",
    "      dataset_text_field=\"formatted_chat\",\n",
    "      max_seq_length=max_seq_length,\n",
    "      tokenizer=tokenizer,\n",
    "      args=training_arguments,\n",
    "      packing=packing,\n",
    "  )\n",
    "\n",
    "  # Setup Wandb\n",
    "  wandb.init(project='TRACHI_Llama', entity='norygano', config=training_arguments.to_dict())\n",
    "\n",
    "  # Start training and let SFTTrainer handle evaluation\n",
    "  trainer.train()\n",
    "\n",
    "  # Finish Wandb session\n",
    "  wandb.finish()\n",
    "\n",
    "  # Save trained model\n",
    "  trainer.model.save_pretrained(new_model)\n",
    "\n",
    "  # Cleanup\n",
    "  del model, trainer\n",
    "  torch.cuda.empty_cache()\n",
    "  gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use unsloth Save when tokenizer is fixed\n",
    "model.save_pretrained_gguf(\"Llama-3-TRACHI-8B-Instruct\", tokenizer, quantization_method = \"q4_k_m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
