{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daphne: Train 99 (0.79), Eval 26 (0.21), Total lines 1057, Avg lines/block 8.46, Avg chars/line 59.91\n",
      "Atlas: Train 137 (0.79), Eval 36 (0.21), Total lines 1674, Avg lines/block 9.68, Avg chars/line 57.05\n",
      "Pandora: Train 35 (0.78), Eval 10 (0.22), Total lines 283, Avg lines/block 6.29, Avg chars/line 55.71\n",
      "Ganymede: Train 222 (0.80), Eval 57 (0.20), Total lines 1982, Avg lines/block 7.10, Avg chars/line 54.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6f398556614b09b414f25d6f7d1bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02b1c197e407406985fc9a806e907b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030f0f1712124c3fae0b0ada4dab3ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563dfda479404587902613bba338722d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde3366971d441e7bff5feb7a9768cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12031e616c7429bbd36656b82fd4f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea65887a7fb043a083ce73839310447a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/430 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/norygano/TRACHI/commit/90d3842cb037e2b5fd34388b481c21c969200f43', commit_message='Upload dataset', commit_description='', oid='90d3842cb037e2b5fd34388b481c21c969200f43', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "def convert_to_jsonl(input_directory, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    train_filename = os.path.join(output_dir, 'train.jsonl')\n",
    "    eval_filename = os.path.join(output_dir, 'eval.jsonl')\n",
    "\n",
    "    # Clear existing data in output files\n",
    "    open(train_filename, 'w').close()\n",
    "    open(eval_filename, 'w').close()\n",
    "\n",
    "    # Dictionary to hold content by bot name\n",
    "    content_by_bot = {}\n",
    "\n",
    "    # Gather files from both 'raw' and 'autonomy' subdirectories\n",
    "    for subfolder in ['raw', 'autonomy']:\n",
    "        subfolder_path = os.path.join(input_directory, subfolder)\n",
    "        for input_filename in os.listdir(subfolder_path):\n",
    "            if input_filename.endswith('.txt'):\n",
    "                bot_name = input_filename.replace('_autonomy', '').replace('.txt', '')\n",
    "                file_path = os.path.join(subfolder_path, input_filename)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                if bot_name not in content_by_bot:\n",
    "                    content_by_bot[bot_name] = content\n",
    "                else:\n",
    "                    content_by_bot[bot_name] += \"\\n\\n\" + content  # Add content from both files\n",
    "\n",
    "    # Process content for each bot\n",
    "    for bot_name, content in content_by_bot.items():\n",
    "        process_conversations(content, bot_name, train_filename, eval_filename)\n",
    "\n",
    "def process_conversations(content, bot_name, train_filename, eval_filename):\n",
    "    conversation_blocks = content.split('\\n\\n')\n",
    "    train_count = 0\n",
    "    eval_count = 0\n",
    "    total_lines = 0\n",
    "    total_characters = 0\n",
    "    block_line_counts = []\n",
    "\n",
    "    for block in conversation_blocks:\n",
    "        lines = block.split('\\n')\n",
    "        total_lines += len(lines)\n",
    "        block_line_counts.append(len(lines))\n",
    "\n",
    "        # Calculate total characters in each line\n",
    "        for line in lines:\n",
    "            total_characters += len(line)\n",
    "\n",
    "        if lines[0].startswith('[t]'):\n",
    "            target_file = train_filename\n",
    "            train_count += 1\n",
    "            lines[0] = lines[0][3:]  # Remove the [t] flag\n",
    "        elif lines[0].startswith('[e]'):\n",
    "            target_file = eval_filename\n",
    "            eval_count += 1\n",
    "            lines[0] = lines[0][3:]  # Remove the [e] flag\n",
    "        else:\n",
    "            continue  # Skip blocks without valid flags\n",
    "\n",
    "        chat = extract_chats(lines, bot_name)\n",
    "        write_jsonl(chat, target_file)\n",
    "\n",
    "    # Calculate and print the training/evaluation ratio for this character\n",
    "    total_blocks = train_count + eval_count\n",
    "    if total_blocks > 0:\n",
    "        train_ratio = train_count / total_blocks\n",
    "        eval_ratio = eval_count / total_blocks\n",
    "        avg_lines_per_block = total_lines / total_blocks\n",
    "        avg_chars_per_line = total_characters / total_lines\n",
    "        print(f\"{bot_name}: Train {train_count} ({train_ratio:.2f}), Eval {eval_count} ({eval_ratio:.2f}), Total lines {total_lines}, Avg lines/block {avg_lines_per_block:.2f}, Avg chars/line {avg_chars_per_line:.2f}\")\n",
    "\n",
    "def extract_chats(lines, bot_name):\n",
    "    chat = [{\"role\": \"system\", \"content\": bot_name}]\n",
    "    for line in lines:\n",
    "        if line.startswith(\"HUMAN:\"):\n",
    "            chat.append({\"role\": \"user\", \"content\": line.replace(\"HUMAN:\", \"\").strip()})\n",
    "        elif line.startswith(\"RESPONSE:\"):\n",
    "            chat.append({\"role\": \"assistant\", \"content\": line.replace(\"RESPONSE:\", \"\").strip()})\n",
    "\n",
    "    return [{\"chat\": chat}] if len(chat) > 1 else []\n",
    "\n",
    "def write_jsonl(chats, filename):\n",
    "    with open(filename, 'a', encoding='utf-8') as jsonl_file:\n",
    "        for chat in chats:\n",
    "            json.dump(chat, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "\n",
    "# Configuration\n",
    "base_dir = os.getcwd()\n",
    "repo_dir = 'datasets/TRACHI'\n",
    "input_directory = os.path.join(base_dir, 'datasets')\n",
    "output_dir = os.path.join(base_dir, repo_dir)\n",
    "\n",
    "# Convert txt files to jsonl files\n",
    "convert_to_jsonl(input_directory, output_dir)\n",
    "\n",
    "dataset = load_dataset(repo_dir)\n",
    "dataset.push_to_hub(\"norygano/TRACHI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Training Blocks: 143\n",
      "Current Evaluation Blocks: 39\n",
      "Current Training Ratio: 0.79\n",
      "Add more training blocks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio Check (jsonl)\n",
    "def manage_ratio(input_directory, expected_ratio=0.8):\n",
    "    train_count = 0\n",
    "    eval_count = 0\n",
    "\n",
    "    # Count existing training and evaluation blocks\n",
    "    for input_filename in os.listdir(input_directory):\n",
    "        if input_filename.endswith('.txt'):\n",
    "            file_path = os.path.join(input_directory, input_filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                blocks = file.read().split('\\n\\n')\n",
    "                for block in blocks:\n",
    "                    if block.startswith('[t]'):\n",
    "                        train_count += 1\n",
    "                    elif block.startswith('[e]'):\n",
    "                        eval_count += 1\n",
    "\n",
    "    total_blocks = train_count + eval_count\n",
    "    current_train_ratio = train_count / total_blocks if total_blocks > 0 else 0\n",
    "\n",
    "    print(f\"Current Training Blocks: {train_count}\")\n",
    "    print(f\"Current Evaluation Blocks: {eval_count}\")\n",
    "    print(f\"Current Training Ratio: {current_train_ratio:.2f}\")\n",
    "\n",
    "    # Determine how many new blocks to add based on the expected ratio\n",
    "    if current_train_ratio < expected_ratio:\n",
    "        print(\"Add more training blocks.\")\n",
    "    else:\n",
    "        print(\"Add more evaluation blocks.\")\n",
    "\n",
    "    return current_train_ratio\n",
    "\n",
    "# Call the function\n",
    "base_dir = os.getcwd()\n",
    "repo_dir = 'datasets/TRACHI'\n",
    "input_directory = os.path.join(base_dir, 'datasets', 'raw')\n",
    "manage_ratio(input_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogues saved to 'datasets/autonomy/Daphne_autonomy.txt'.\n",
      "Dialogues saved to 'datasets/autonomy/Atlas_autonomy.txt'.\n",
      "Dialogues saved to 'datasets/autonomy/Lorna_autonomy.txt'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(file_path):\n",
    "    # Load the CSV with specified delimiter and escape character\n",
    "    return pd.read_csv(file_path, delimiter='\\t', escapechar='\\\\')\n",
    "\n",
    "# Define character aliases\n",
    "character_aliases = {\n",
    "    \"Eurydice\": ['Eurydice'],\n",
    "}\n",
    "\n",
    "df = load_csv('AUTONOMY.csv')\n",
    "\n",
    "def process_character_dialogues(character, aliases, df):\n",
    "    # Identify conversations that include the character\n",
    "    df['Character_present'] = df['Speaker'].isin(aliases)\n",
    "    relevant_maps = df[df['Character_present']]['Map'].unique()\n",
    "    filtered_df = df[df['Map'].isin(relevant_maps)].copy()\n",
    "    \n",
    "    output_path = f\"datasets/autonomy/{character}_autonomy.txt\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        current_map = None\n",
    "        last_speaker = None\n",
    "\n",
    "        for _, row in filtered_df.iterrows():\n",
    "            if row['Map'] != current_map:\n",
    "                # Close the last conversation with RESPONSE if needed\n",
    "                if current_map is not None and last_speaker not in aliases:\n",
    "                    f.write(\"\\nRESPONSE:\")\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                current_map = row['Map']\n",
    "                last_speaker = None  # Reset the last speaker for new map\n",
    "                # Start with an empty HUMAN if the first speaker is the character\n",
    "                if row['Speaker'] in aliases:\n",
    "                    f.write(\"HUMAN:\\n\")\n",
    "\n",
    "            if last_speaker != row['Speaker']:\n",
    "                if last_speaker is not None:\n",
    "                    f.write(\"\\n\")\n",
    "                f.write(\"RESPONSE: \" if row['Speaker'] in aliases else \"HUMAN: \")\n",
    "            f.write(row['Dialogue'].strip() + \" \")\n",
    "\n",
    "            last_speaker = row['Speaker']\n",
    "\n",
    "        # Ensure the last dialogue in the file ends with RESPONSE:\n",
    "        if last_speaker not in aliases:\n",
    "            f.write(\"\\nRESPONSE: \")\n",
    "\n",
    "    print(f\"Dialogues saved to '{output_path}'.\")\n",
    "\n",
    "# Process dialogues for each character\n",
    "for character, aliases in character_aliases.items():\n",
    "    process_character_dialogues(character, aliases, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD: Notate\n",
    "import os\n",
    "from random import sample, seed\n",
    "\n",
    "def add_split_notation(input_directory, split_ratio=0.8, random_seed=42):\n",
    "    seed(random_seed)\n",
    "\n",
    "    for input_filename in os.listdir(input_directory):\n",
    "        if input_filename.endswith('Atlas_autonomy.txt'):\n",
    "            file_path = os.path.join(input_directory, input_filename)\n",
    "\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                conversation_blocks = file.read().split('\\n\\n')\n",
    "                \n",
    "            total_blocks = len(conversation_blocks)\n",
    "            split_index = int(total_blocks * split_ratio)\n",
    "            \n",
    "            # Generate a random sample of indexes for training\n",
    "            training_indexes = set(sample(range(total_blocks), split_index))\n",
    "            \n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                for i, block in enumerate(conversation_blocks):\n",
    "                    lines = block.split('\\n')\n",
    "                    \n",
    "                    if i in training_indexes:\n",
    "                        lines[0] = \"[t]\" + lines[0]\n",
    "                    else:\n",
    "                        lines[0] = \"[e]\" + lines[0]\n",
    "                    \n",
    "                    # Join the lines back together and write back to the file\n",
    "                    file.write('\\n'.join(lines) + '\\n\\n')\n",
    "\n",
    "# Configuration\n",
    "base_dir = os.getcwd()\n",
    "repo_dir = 'datasets/TRACHI'\n",
    "input_directory = os.path.join(base_dir, 'datasets', 'autonomy')\n",
    "\n",
    "# Run the function\n",
    "add_split_notation(input_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
