{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert Dataset (jsonl) -> fine-tune dolphin-mistal-7b -> quantize -> create ollama Modelfile -> push .gguf (HF)\n",
    "\n",
    "TODO:\n",
    "- Convert .ipynb to py\n",
    "  - dataset.py | train.py\n",
    "- Create config.yaml\n",
    "  - Base Model + Dataset\n",
    "  - Training parameters\n",
    "  - Quantization Methods\n",
    "- Check Discord implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "954aa77a7e3e4e759c25dfdc768a7342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_jsonl(input_directory, output_filename):\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    \n",
    "    all_chats = []  # To hold all chats before writing to JSONL\n",
    "\n",
    "    for input_filename in os.listdir(input_directory):\n",
    "        if input_filename.endswith('.txt'):\n",
    "            bot_name = input_filename[:-4]  # Remove '.txt' from filename to get bot_name\n",
    "            file_path = os.path.join(input_directory, input_filename)\n",
    "                \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                conversation_blocks = file.read().split('\\n\\n')\n",
    "                \n",
    "            for block in conversation_blocks:\n",
    "                lines = block.split('\\n')\n",
    "                chat = [{\"role\": \"system\", \"content\": bot_name}]\n",
    "                for line in lines:\n",
    "                    if line.startswith(\"HUMAN:\"):\n",
    "                        chat.append({\"role\": \"user\", \"content\": line.replace(\"HUMAN:\", \"\").strip()})\n",
    "                    elif line.startswith(\"RESPONSE:\"):\n",
    "                        chat.append({\"role\": \"assistant\", \"content\": line.replace(\"RESPONSE:\", \"\").strip()})\n",
    "\n",
    "                if len(chat) > 1:  # Ensure there's more than just the system line\n",
    "                    all_chats.append({\"chat\": chat})\n",
    "    \n",
    "    # Writing all chats to a single JSONL file\n",
    "    with open(output_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for chat in all_chats:\n",
    "            json.dump(chat, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "\n",
    "# Configuration\n",
    "base_dir = 'F:/discollama'\n",
    "repo_dir = 'datasets/TRACHI'\n",
    "input_directory = os.path.join(base_dir, 'datasets', 'raw')\n",
    "output_filename = os.path.join(base_dir, repo_dir, 'train.jsonl')\n",
    "\n",
    "# Convert txt files to a single jsonl file\n",
    "convert_to_jsonl(input_directory, output_filename)\n",
    "\n",
    "subprocess.run([\"huggingface-cli\", \"login\", os.getenv('HUGGINGFACE_TOKEN')])\n",
    "dataset = load_dataset(repo_dir)\n",
    "dataset.push_to_hub(\"norygano/TRACHI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
