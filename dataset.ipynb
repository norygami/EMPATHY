{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train llama7B-chat-uncensored using Dataset -> Push Output (Ganymede) to HF\n",
    "- Quantize Ganymede -> Push Ganymede.GGUF to HF\n",
    "- Adjust Modelfile to use Ganymede.GGUF -> ollama create Ganymede -f Modelfile_Ganymede\n",
    "- Check Discord implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from datasets import load_dataset\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_jsonl(input_directory, output_filename):\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    \n",
    "    with open(output_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for input_filename in os.listdir(input_directory):\n",
    "            if input_filename.endswith('.txt'):\n",
    "                bot_name = input_filename[:-4]  # Remove '.txt' from filename to get bot_name\n",
    "                file_path = os.path.join(input_directory, input_filename)\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    conversation_blocks = file.read().split('\\n\\n')\n",
    "                \n",
    "                for block in conversation_blocks:\n",
    "                    lines = block.split('\\n')\n",
    "                    question_lines = []\n",
    "                    response_lines = []\n",
    "\n",
    "                    for line in lines:\n",
    "                        if line.startswith(\"HUMAN:\"):\n",
    "                            question_lines.append(line.replace(\"HUMAN:\", \"\").strip())\n",
    "                        elif line.startswith(\"RESPONSE:\"):\n",
    "                            response_lines.append(line.replace(\"RESPONSE:\", \"\").strip())\n",
    "\n",
    "                    if question_lines and response_lines:\n",
    "                        # Construct the entry\n",
    "                        entry = {\n",
    "                            \"system\": f\"{bot_name}\",\n",
    "                            \"question\": \" \".join(question_lines),\n",
    "                            \"response\": \" \".join(response_lines)\n",
    "                        }\n",
    "                        json.dump(entry, jsonl_file)\n",
    "                        jsonl_file.write('\\n')\n",
    "\n",
    "# Configuration\n",
    "base_dir = 'F:/discollama'\n",
    "repo_dir = 'datasets/TRACHI'\n",
    "input_directory = os.path.join(base_dir, 'datasets', 'raw')\n",
    "output_filename = os.path.join(base_dir, repo_dir, 'train.jsonl')\n",
    "\n",
    "# Convert txt files to a single jsonl file\n",
    "convert_to_jsonl(input_directory, output_filename)\n",
    "\n",
    "subprocess.run([\"huggingface-cli\", \"login\", os.getenv('HUGGINGFACE_TOKEN')])\n",
    "dataset = load_dataset(repo_dir)\n",
    "dataset.push_to_hub(\"norygano/TRACHI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F:\\discollama\n",
      "Pushing F:/discollama\\datasets\\Daphne to norygano/Daphne\n",
      "Pushing F:/discollama\\datasets\\Ganymede to norygano/Ganymede\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import datasets\n",
    "import huggingface_hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "# List of bot names\n",
    "bot_names = ['Daphne', 'Ganymede']\n",
    "\n",
    "\n",
    "def convert_to_jsonl(input_filename, output_filename):\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    with open(input_filename, 'r', encoding='utf-8') as file:\n",
    "        conversation_blocks = file.read().split('\\n\\n')\n",
    "\n",
    "    conversations = []\n",
    "\n",
    "    for block in conversation_blocks:\n",
    "        # Initialize formatted_block for accumulating the conversation\n",
    "        formatted_block = \"\"\n",
    "        lines = block.split('\\n')\n",
    "\n",
    "        for line in lines:\n",
    "            # Directly append \"### \" to lines starting with \"HUMAN:\" or \"RESPONSE:\"\n",
    "            if line.startswith(\"HUMAN:\") or line.startswith(\"RESPONSE:\"):\n",
    "                formatted_block += f\"### {line}\\n\\n\"\n",
    "            else:\n",
    "                # Handle any additional text that might be part of the response\n",
    "                formatted_block += line + \"\\n\\n\"\n",
    "\n",
    "        if formatted_block:\n",
    "            # Ensure the block is properly formatted before appending\n",
    "            formatted_block = formatted_block.strip() + \"\\n\\n\"\n",
    "            conversations.append({\"text\": formatted_block})\n",
    "\n",
    "    with open(output_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "        for conversation in conversations:\n",
    "            json.dump(conversation, jsonl_file)\n",
    "            jsonl_file.write('\\n')\n",
    "\n",
    "def git_commit_push(repo_path, file_name, commit_message):\n",
    "    os.chdir(repo_path)  # Change working directory to repo path\n",
    "    subprocess.run([\"git\", \"pull\"])  # Pull latest changes\n",
    "    subprocess.run([\"git\", \"add\", file_name])  # Add the .jsonl file to staging\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])  # Commit changes\n",
    "    subprocess.run([\"git\", \"push\"])  # Push changes to remote\n",
    "    os.chdir(base_dir)\n",
    "\n",
    "def hf_push(repo_path):\n",
    "    # Ensure we're in the correct directory\n",
    "    os.chdir(repo_path)\n",
    "    \n",
    "    # Login to HF\n",
    "    subprocess.run([\"huggingface-cli\", \"login\", os.getenv('HUGGINGFACE_TOKEN')])\n",
    "    \n",
    "    # Push\n",
    "    hf_repo_path = f'norygano/{bot_name}'\n",
    "    print(f'Pushing {repo_path} to {hf_repo_path}')\n",
    "    api.upload_folder(\n",
    "\n",
    "    folder_path=repo_path,\n",
    "\n",
    "    repo_id=hf_repo_path,\n",
    "\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#References\n",
    "base_dir = 'F:/discollama'  # Adjust base directory as necessary\n",
    "os.chdir(base_dir)\n",
    "print(os.getcwd())\n",
    "\n",
    "for bot_name in bot_names:\n",
    "    repo_path = os.path.join(base_dir, 'datasets', bot_name)\n",
    "    raw_input_path = os.path.join(base_dir, 'datasets', 'raw', f'{bot_name}.txt')\n",
    "    output_filename = f'train.jsonl'\n",
    "    output_path = os.path.join(repo_path, output_filename)\n",
    "    commit_message = f'Updated dataset with new conversations for {bot_name}'\n",
    "\n",
    "    # Convert txt to jsonl\n",
    "    convert_to_jsonl(raw_input_path, output_path)\n",
    "    \n",
    "    # Commit and push the changes\n",
    "    git_commit_push(repo_path, output_filename, commit_message)\n",
    "    \n",
    "    # Push to Hugging Face\n",
    "    hf_push(repo_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#--- Writing Template ---\n",
    "HUMAN: Can you explain the significance of the Turing Test in artificial intelligence?\n",
    "RESPONSE: The Turing Test, proposed by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behavior equivalent to, or indistinguishable from, that of a human. According to the test, if a human judge cannot reliably tell the machine from a human based on their responses to questions, the machine is considered to have passed the test. This test has been significant in discussions of artificial intelligence as it focuses on a machine's ability to simulate human-like intelligence, rather than just executing tasks. It raises questions about what it means to think and whether a machine can possess qualities like consciousness or understanding.\n",
    "\n",
    "HUMAN: [Next Prompt]\n",
    "RESPONSE: [Next Response]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distinction between params and template in the context of using a model like the one from the URL you've mentioned relates to how you configure the model for generating responses and how you format the input data for those responses.\n",
    "Params\n",
    "\n",
    "The params specify certain configurations or settings that control the model's behavior during inferenceâ€”i.e., when it's generating responses. In your case:\n",
    "\n",
    "    stop is an array of strings that tells the model when to stop generating text. It's like saying, \"If you encounter any of these strings (\"### HUMAN:\", \"### RESPONSE:\"), consider your response complete and stop generating more text.\" This is useful for ensuring that the model's output is bounded and doesn't go beyond the expected response format.\n",
    "\n",
    "Template\n",
    "\n",
    "The template is a format string that defines how the input (prompt) to the model should be structured. It's a way of telling the model, \"Here's how I'll present the data to you, and here's where you come in to generate a response.\" The placeholders ({{ .System }}, {{ .Prompt }}) are replaced with actual values at runtime. In this case:\n",
    "\n",
    "    {{ .System }} might be replaced with any system-level instructions or context needed for generating a response.\n",
    "    ### HUMAN: indicates where the human's part of the conversation starts.\n",
    "    {{ .Prompt }} is where the actual user query or statement goes.\n",
    "    ### RESPONSE: is where the model's generated response will be placed.\n",
    "\n",
    "To rewrite the initial script to accommodate the model's params/template, you would adjust the transformation function to format the dataset according to the template structure expected by the model. Here's how you might adjust the transform_conversation function:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
